---
title: "Assignment 3"
author: "Filip Mellgren"
date: '2019-02-23'
output: 
  pdf_document:
    df_print: tibble
    highlight: tango
    toc: true
---
---


Hello,this is a comment from Ismael

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```

# Theoretical exercises
## 1
### a: 
test
Show that $* = Cov(z_t, \varepsilon_{yt}) \neq 0$.

- Recall the formula for covariance: $Cov(z_t, \varepsilon_{yt}) = E(z_t\varepsilon_{yt})-E(z_t)E(\varepsilon_{yt})$. Because $\varepsilon_{yt} \sim WN(0,\sigma_y^2)$, we obtain: $* = E(z_t\varepsilon_{yt})$. 

- Next, expand the the expression for $y_t$ in the expression for $z_t$: $* = E[(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}] + \gamma_{21}y_{t-1}+\gamma_{22}z_{t-1} + \varepsilon_{zt})\varepsilon_{yt}]$. 

- Now distribute $\varepsilon_{yt}$ over the system: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt} + \gamma_{21}y_{t-1}\varepsilon_{yt} + \gamma_{22}z_{t-1}\varepsilon_{yt} + \varepsilon_{zt}\varepsilon_{yt})$

- Expand the expectation operator to a sum: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt}) + E(\gamma_{21}y_{t-1}\varepsilon_{yt}) + E(\gamma_{22}z_{t-1}\varepsilon_{yt}) + E(\varepsilon_{zt}\varepsilon_{yt})$.


- Exploit intertemporal independence and that $\varepsilon_{yt}$ and $\varepsilon_{zt}$ are independent: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt})$

- Distibute $\varepsilon_{yt}$: $* = -b_{21}E([(b_{12}z_t\varepsilon_{yt} + \gamma_{11}y_{t-1}\varepsilon_{yt} + \gamma_{12}z_{t-1}\varepsilon_{yt} + \varepsilon_{yt}\varepsilon_{yt}])$

- Expand the expectation: $*=-b_{21}[E(b_{12}z_t\varepsilon_{yt})+ E(\gamma_{11}y_{t-1}\varepsilon_{yt}) + E(\gamma_{12}z_{t-1}\varepsilon_{yt}) + E(\varepsilon_{yt}^2)]$

- What remains after exploiting independence is $*=-b_{21} E(\varepsilon_{yt}^2) = -b_{21}\sigma_y^2 \neq 0$ QED. 

The implications on estimation are that estimates will be inefficient and baised.

###b

Firstly, we express (1) in the following matrix form:

$$BX_t= \Gamma_0 + \Gamma_1X_{t-1} + \varepsilon_t$$
Where \[
   B=
  \left[ {\begin{array}{cc}
   1 & b_{12} \\
   b_{21} & 1 \\
  \end{array} } \right]
\] , 
\[
   X_t=
  \left[ {\begin{array}{cc}
   y_t  \\
   z_t \\
  \end{array} } \right]
\]
\[
   \Gamma_0=
  \left[ {\begin{array}{cc}
   b_{10}  \\
   b_{20} \\
  \end{array} } \right]
\]
\[
   \Gamma_1=
  \left[ {\begin{array}{cc}
   \gamma_{11} & \gamma_{12} \\
   \gamma_{21} & \gamma_{22} \\
  \end{array} } \right]
\]



\[
   \Gamma_0=
  \left[ {\begin{array}{cc}
   b_{10}  \\
   b_{20} \\
  \end{array} } \right]
\]


### d
```{r fig.height = 4, fig.width = 4}
#d a i
# calculate inverse
b21 <- 0
b12 <- -0.8
B <- cbind(c(1, b21), c(b12, 1))
B_inv <- solve(B)
A <- cbind(c(.6, .1), c(.2, .8))

p1 <- B_inv %*% c(1,0)
p2 <- A%*%p1
p3 <- A%*%p2
p4 <- A%*%p3
series <- t(cbind(p1, p2, p3, p4))
plot(series[,1], xlab = "Iteration", ylab = "Impact")
plot(series[,2], xlab = "Iteration", ylab = "Impact")

```
```{r fig.height = 4, fig.width = 4}
#d a ii
# calculate inverse
b21 <- 0
b12 <- -0.8
B <- cbind(c(1, b21), c(b12, 1))
B_inv <- solve(B)
A <- cbind(c(.6, .1), c(.2, .8))

p1 <- B_inv %*% c(0,1)
p2 <- A%*%p1
p3 <- A%*%p2
p4 <- A%*%p3
series <- t(cbind(p1, p2, p3, p4))
plot(series[,1], xlab = "Iteration", ylab = "Impact")
plot(series[,2], xlab = "Iteration", ylab = "Impact")

```
There's a unit root if solutions of the system $det(I - zA_1) = 0$ lie on the unit circle. Take $z = 1$, then 
$$I - A_1 =
\left[ {\begin{array}{cc} 
1- a_{11} & -a_{12} \\ 
-a_{21} & 1-a_{22} \\ 
\end{array} } \right]$$
 and the determinant is: 

## 2

# Empirical exercises
Do exercises 10a-10g in the textbook (p.340)

- Remark 1: It is possible that the
values you obtain for the F-statistics, p-values and correlations are different than those
reported since the sample is extended. However, the main conclusions should be the same. 

- Remark 2: Exercise d. is optional and so is the part on the forecast error variance in e. (but you could use the command fevd in STATA to answer these questions).

- Remark 3: You find the appropriate specifications for the variables st
, $\Delta$lip, and $\Delta$ur described in the text to exercise 9 (p.339).

```{r, include = FALSE}
# Load necessary packages
library(rio)
library(tidyverse)
library(vars)
library(broom)
library(stargazer)
```


### 10: 
Estimate the three-VAR beginning in 1961Q1 and use the ordering such that $\Delta lip_t$ is causally prior to $\Delta ur_t$ and that $\Delta ur_t$ is causally prior to $s_t$.
```{r}
# Load the data
df <- as_tibble(import("A3_2019_data.dta"))
```

We begin by defining the variables we are going to include in our analysis. We create $dlip = log(indprod_t) - log(indprod_{t-1}), dur = urate_t - urate_{t-1}$ and $s = r10 - tbill$.
```{r}
# Make the date variable a number
df <- df %>% mutate(year = as.numeric(str_sub(DATE, start = 1, end = 4)),
                Q = as.numeric(str_sub(DATE,-1, -1)),
                DATE = year + (Q-1)/4)

# Need to define some new variables as well
df <- df %>% mutate(dlip = log(indprod)-log(lag(indprod)), 
                    dur = urate - lag(urate),
                    s = r10 - tbill)
```
In the context of chapter 5, we assume that staionarity holds. Additionally, it is provided for us that the appropriate lag length is 3. The result of the var estimation is as follows:
```{r}
# Eventually, stationarity checks.
```

```{r VAR}
# Estimate the 3 VAR, with the ordering dlip, dur, s
# Page 310 and forward looks useful. Also around 292

var <- df %>% dplyr::select(s, dur, dlip) %>% dplyr::filter(row_number() > 1) %>% as.matrix() %>% VAR(p = 3, type = "none")
summary(var)
```

### 10 a:
If you perform a test to determine whether $s_t$ Granger causes $\Delta lip_t$, you should find that the F-statistic is 2.44 with a prob-value of 0.065. How do you interpret this result?
```{r results = "asis"}
# might use grangertest()
#grangertest(x = df$dlip, y = df$s, order = 3) # Not a similar value
gc_sdlip <- grangertest(x = df$s, y = df$dlip, order = 3) # Somewhat similar
stargazer(gc_sdlip, header = FALSE, summary.stat = c("n", "mean", "sd"), type = "latex", float=FALSE, iqr = FALSE, median = FALSE)
```
The p-value is borderline significant. Assuming the null that no lag of $s$ predicts $dlip$ does not hold, then the meaning is that there is a lag of $s$ that does predict $dlip$. Hence, $s$ granger causes $dlip$. However, it is not clear cut given the p-value.

### 10 b:
Verify that $s_t$ Granger causes $\Delta unemp_t$. You should find that the F statistic is 5.93 with a prob value of less than 0.001.
```{r results = "asis"}
gc_sdur <- grangertest(x = df$s, y = df$dur, order = 3)
stargazer(gc_sdur, header = FALSE, summary.stat = c("n", "mean", "sd"),type = "latex", float=FALSE)
```

### 10 c:
It turns out that the correlation coefficient between $e_{1t}$ and $e_{2t}$ is -0.72. The correlation between $e_{1t}$ and $e_{3t}$ is -0.11 and between $e_{2t}$ and $e_{3t}$ is 0.10. Explain why the ordering of a Choleski composition is likely to be important for obtaining the impulse responses.


### 10 e:
Now estimate the model using the levels of $lip_t$ and $ur_t$. Do you now find a lag length of 5 appropriate?

```{r}
df <- df %>% mutate(lip = log(indprod))
# Select the variables in the correct order (for the cholesky decomp to be 
# right). Filter away first row to not have any NA (created when differencing)
 
var_e <- df %>% dplyr::select(s, dur, dlip) %>% dplyr::filter(row_number() > 1) %>% as.matrix() %>% VAR(p = 5, type = "none")
summary(var_e)
```
We do not think a lag length of 5 is appropriate as the AIC suggests a lag length of 1 is sufficient. 

### 10 f:
Obtain the impulse response function from the model using $\Delta lip_t, \Delta ur_t$ and $s_t$. Show that a positive shock to the industrial production induces a decline in the unemployment rate that lasts six quarters. Then, $\Delta ur_t$ overshoots its long run level before returning to zero.

```{r fig.height = 3, fig.width = 5}
irf <- irf(var, ortho = TRUE, n.ahead = 10)
plot(irf)
```


### 10 g: 
Reverse the ordering and explain why the results depend on whether or not $\Delta lip_t$ proceeds $\Delta ur_t$

```{r}
# Select the variables in the correct order (for the cholesky decomp to be 
# right). Filter away first row to not have any NA (created when differencing)
 
var_g <- df %>% dplyr::select(s, dur, dlip) %>% dplyr::filter(row_number() > 1) %>% as.matrix() %>% VAR(p = 3, type = "none")
irf_g <- irf(var_g, ortho = TRUE, n.ahead = 10)
```

```{r, fig.height = 3, fig.width = 5}
plot(irf_g)
```

If $\Delta lip_t$ proceeds $\Delta ur_t$, then a contemporary effect on $\Delta lip_t$ affects $\Delta ur_t$, but not vice versa. On the other hand, if the reverse holds, then shocks to $\Delta lip_t$ will be delayed until next period before any noticeable change occurs to $\Delta ur_t$.
