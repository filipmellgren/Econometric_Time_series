---
title: "Assignment 3"
author: "Filip Mellgren"
date: '2019-02-23'
output: 
  pdf_document:
    df_print: tibble
    highlight: tango
    toc: true
---

#This is a comment by Ismael

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```

# Theoretical exercises
## 1
### a: 
test
Show that $* = Cov(z_t, \varepsilon_{yt}) \neq 0$.

- Recall the formula for covariance: $Cov(z_t, \varepsilon_{yt}) = E(z_t\varepsilon_{yt})-E(z_t)E(\varepsilon_{yt})$. Because $\varepsilon_{yt} \sim WN(0,\sigma_y^2)$, we obtain: $* = E(z_t\varepsilon_{yt})$. 

- Next, expand the the expression for $y_t$ in the expression for $z_t$: $* = E[(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}] + \gamma_{21}y_{t-1}+\gamma_{22}z_{t-1} + \varepsilon_{zt})\varepsilon_{yt}]$. 

- Now distribute $\varepsilon_{yt}$ over the system: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt} + \gamma_{21}y_{t-1}\varepsilon_{yt} + \gamma_{22}z_{t-1}\varepsilon_{yt} + \varepsilon_{zt}\varepsilon_{yt})$

- Expand the expectation operator to a sum: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt}) + E(\gamma_{21}y_{t-1}\varepsilon_{yt}) + E(\gamma_{22}z_{t-1}\varepsilon_{yt}) + E(\varepsilon_{zt}\varepsilon_{yt})$.


- Exploit intertemporal independence and that $\varepsilon_{yt}$ and $\varepsilon_{zt}$ are independent: $* = E(-b_{21}[(b_{12}z_t + \gamma_{11}y_{t-1} + \gamma_{12}z_{t-1} + \varepsilon_{yt}]\varepsilon_{yt})$

- Distibute $\varepsilon_{yt}$: $* = -b_{21}E([(b_{12}z_t\varepsilon_{yt} + \gamma_{11}y_{t-1}\varepsilon_{yt} + \gamma_{12}z_{t-1}\varepsilon_{yt} + \varepsilon_{yt}\varepsilon_{yt}])$

- Expand the expectation: $*=-b_{21}[E(b_{12}z_t\varepsilon_{yt})+ E(\gamma_{11}y_{t-1}\varepsilon_{yt}) + E(\gamma_{12}z_{t-1}\varepsilon_{yt}) + E(\varepsilon_{yt}^2)]$

- What remains after exploiting independence is $*=-b_{21} E(\varepsilon_{yt}^2) = -b_{21}\sigma_y^2 \neq 0$ QED. 

The implications on estimation are that estimates will be inefficient and baised.
## 2

# Empirical exercises
Do exercises 10a-10g in the textbook (p.340)

- Remark 1: It is possible that the
values you obtain for the F-statistics, p-values and correlations are different than those
reported since the sample is extended. However, the main conclusions should be the same. 

- Remark 2: Exercise d. is optional and so is the part on the forecast error variance in e. (but you could use the command fevd in STATA to answer these questions).

- Remark 3: You find the appropriate specifications for the variables st
, $\Delta$lip, and $\Delta$ur described in the text to exercise 9 (p.339).

```{r}
# Load necessary packages
library(rio)
library(tidyverse)
library(vars)
library(broom)
library(stargazer)
```


### 10: 
Estimate the three-VAR beginning in 1961Q1 and use the ordering such that $\Delta lip_t$ is causally prior to $\Delta ur_t$ and that $\Delta ur_t$ is causally prior to $s_t$.
```{r}
# Load the data
df <- as_tibble(import("A3_2019_data.dta"))
```

We begin by defining the variables we are going to include in our analysis.
```{r}
# Make the date variable a number
df <- df %>% mutate(year = as.numeric(str_sub(DATE, start = 1, end = 4)),
                Q = as.numeric(str_sub(DATE,-1, -1)),
                DATE = year + (Q-1)/4)

# Need to define some new variables as well
df <- df %>% mutate(dlip = log(indprod)-log(lag(indprod)), 
                    dur = urate - lag(urate),
                    s = r10 - tbill)
```
Do we need to check for stationarity here? Might be enough to assume it.
```{r}
# Eventually, stationarity checks.
```
The lag length is already determined to be 3. 


```{r VAR}
# Estimate the 3 VAR, with the ordering dlip, dur, s
# Page 310 and forward looks useful. Also around 292
# Do we first estimate the standard VARs and then go to identify the parameters in the structural VAR?
dlip_fit <- lm(dlip ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3), data = df)
dur_fit <- lm(dur ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3), data = df)
s_fit <- lm(s ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3), data = df)
```


```{r altVAR}
# This is an alternative to VAR above. 
# The idea is to estimate the system akin to 5.17 & 5.18 in the book
# with the imposed restrictions.

dlip_fit <- lm(dlip ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3),
               data = df)

dur_fit <- lm(dur ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3) + 
                dlip, 
              data = df[5:235,])

s_fit <- lm(s ~ lag(dlip) + lag(dlip,2) + lag(dlip,3) + 
                 lag(dur) + lag(dur,2) + lag(dur,3) + 
                 lag(s) + lag(s,2) + lag(s,3) +
              dlip + dur, 
            data = df)


```

### 10 a:
If you perform a test to determine whether $s_t$ Granger causes $\Delta lip_t$, you should find that the F-statistic is 2.44 with a prob-value of 0.065. How do you interpret this result?
```{r results = "asis"}
# might use grangertest()
#grangertest(x = df$dlip, y = df$s, order = 3) # Not a similar value
gc_sdlip <- grangertest(x = df$s, y = df$dlip, order = 3) # Somewhat similar
stargazer(gc_sdlip, header = FALSE, omit = "school", type = "latex", float=FALSE)
```

### 10 b:
Verify that $s_t$ Granger causes $\Delta unemp_t$. You should find that the F statistic is 5.93 with a prob value of less than 0.001.
```{r results = "asis"}
# might use grangertest()
gc_sdur <- grangertest(x = df$s, y = df$dur, order = 3) # not the same
stargazer(gc_sdur, header = FALSE, omit = "school", type = "latex", float=FALSE)
```

### 10 c:
It turns out that the correlation coefficient between $e_{1t}$ and $e_{2t}$ is -0.72. The correlation between $e_{1t}$ and $e_{3t}$ is -0.11 and between $e_{2t}$ and $e_{3t}$ is 0.10. Explain why the ordering of a Choleski composition is likely to be important for obtaining the impulse responses.


### 10 e:
Now estimate the model using the levels of $lip_t$ and $ur_t$. Do you now find a lag length of 5 appropriate?

```{r}
df <- df %>% mutate(lip = log(indprod))
```

### 10 f:
Obtain the impulse response function from the model using $\Delta lip_t, \Delta ur_t$ and $s_t$. Show that a positive shock to the industrial production induces a decline in the unemployment rate that lasts six quarters. Then, $\Delta ur_t$ overshoots its long run level before returning to zero.

```{r}
# https://www.rdocumentation.org/packages/vars/versions/1.5-3/topics/irf
```


### 10 g: 
Reverse the ordering and explain why the results depend on whether or not $\Delta lip_t$ proceeds $\Delta ur_t$

```{r}

```


